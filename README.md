## PART 1: Project Overview & Data Preprocessing

This document briefly describes the part of the project implemented by **Beining Bao (BBN)**.  The goal of this component is to bridge the raw WikiMEL resources and all downstream modules (LLM expansion, local model, and MST global optimization) by providing a clean, mention-level dataset and a clear description of the overall pipeline.

### 1. Purpose

The preprocessing and overview module has two main objectives:

- Construct a unified **mention-level dataset** from the WikiMEL text subset, so that all subsequent modules can work on the same standardized input.
- Specify the **overall system workflow and data interfaces**, ensuring that each component (LLM expansion, local scoring, global optimization, evaluation) operates on compatible formats and can be composed into an end‑to‑end OpenMEL-style entity linking pipeline.

### 2. Main Output

The primary artifact of this module is:

- **`data/mentions.json`**: A cleaned and structured collection of mention instances extracted from the WikiMEL corpus. Each record corresponds to one textual mention together with its basic information and supervision signal.

This file serves as the **single source of truth** for all subsequent stages:

- The **LLM expansion module** reads `data/mentions.json` and generates`data/mentions_expanded.jsonl`.
- The **local BERT model** consumes the original and expanded mentions for scoring.
- The **MST-based global optimizer and the evaluation scripts** operate on the predictions that ultimately trace back to this preprocessed file.

### 3. High-Level Data Description

Each entry in `data/mentions.json` represents one mention in context.  At a high level, the information per entry includes:

- Document- or sample-level identifiers (e.g. document ID, mention ID).
- The surface form of the mention and its position in the original text.
- Local textual context around the mention (such as left and right context or full sentence/paragraph).
- Candidate entity information derived from the WikiMEL resources.
- The gold / reference entity label, when available, for training and evaluation.

The exact field names and JSON structure are defined consistently with the overall project report’s “Project Overview & Data Format” section, so that downstream scripts can parse the file without additional assumptions.

### 4. Relation to the Overall Pipeline

Within the full OpenMEL-style system, this module plays the role of**input standardization and documentation**:

- It transforms raw WikiMEL text and annotations into a single, coherent mention-level dataset (`data/mentions.json`).
- It documents the global architecture of the project (data flow from preprocessing → LLM expansion → local model → MST optimization → evaluation), providing a reference for both implementation and experimentation.
- It guarantees that all team members work on aligned assumptions about input and output formats, which is essential for integrating independently developed modules into one end‑to‑end demo.

This README is intended as a concise summary of that functionality and of the central role of `data/mentions.json` in the project.

## PART 2: Text expanding based on LLM

### 1. Overview

This module takes over the basic data generated by the previous member, expands the entity mention context based on the Llama3 big language model, supplements core features such as entity types and key attributes, enhances candidate entity discrimination, and provides high-quality input data for downstream global inference modules.

### 2. Model selection

   `llama3:8b-direct-q4_K_M`

### 3. Data source

   The `mention.json` file which contains 12 typical ambiguous entity mentions.

### 4. Core operational steps

- Prompt design which has core constraints as follows:
  - Clearly label entity types.
  - Add 2-3 core attributes of entities.
  - Link with the original context.
  - Retain the core nouns of the original text.
  - Control the expansion length.
- Batch expansion execution
  - Load the `evention.json` file and verify the validity of the data.
  - Call the local Olama interface to perform batch expansion.
  - Add timeout retry mechanism to ensure the stability of the expansion process.
- Result verification
  - Text length ≥30 words, no blank values.
  - Contains standard entity type labels, core attributes, and original contextual association information.
  - Manually supplement and adjust invalid results to ensure 100% validity of output data.
- After successful verification of the output results, save the expanded content to the file named `mentions_expanded.jsonl`.

## PART 3: Text Local Scoring Model for Entity Linking

### 1. Overview

This repository contains a simplified implementation of the text modality core module from the OpenMEL (Open Multi-modal Entity Linking) framework. The model focuses on computing semantic similarity scores between entity mentions and candidate entities using BERT-based encodings, serving as a fundamental component for entity disambiguation in knowledge base linking tasks.

### 2. Key Features

- **BERT-based Encoding:** Utilizes pre-trained `BERT-base-uncased` model for contextual text representation.
- **Dual-Encoder Architecture:** Independently encodes mention texts and entity descriptions using shared BERT weights.
- **LLM Query Expansion Support:** Incorporates expanded context from LLM for enriched mention representations.
- **Offline Operation:** Supports pre-downloaded BERT models for network-independent deployment.
- **Modular Design:** Follows standardized JSON interfaces for seamless integration with other pipeline components.

###  3. Installation

```
Python 3.7+
PyTorch 1.9+
Transformers 4.20+
```

### 4. Step-by-Step Execution

- **Install required packages** 

```bash
pip install torch transformers numpy scikit-learn
```

- **Download BERT model**

```bash
huggingface-cli download bert-base-uncased --local-dir ./bert-base-uncased --local-dir-use-symlinks
```

- Prepare input data

JSON array with mention objects containing `mention_id`, `mention_text`, `candidates`.

- **Run the scoring model**

```bash
python text_local_scorer.py
```

- **Check results**

Output will be saved to `local_scores.jsonl` with format:

```json
{"mention_id": "m1", "entity_id": "Q41421", "local_score": 0.710}
```

### 5. Reproducibility Assurance

The implementation includes deterministic settings:

- Fixed random seed (42).
- CUDA deterministic algorithms (when GPU available).
- Disabled shuffle in DataLoader during inference.
- Controlled dropout behavior in evaluation mode.

## PART 4: Global Optimization of MST

### 1. Overview

This code implements a graph-based MST optimization system. It constructs an entity graph containing entity mentions and candidate entities using cosine similarity, extracts key connections via maximum spanning tree, and applies a greedy algorithm for global consistency adjustment. The system operates in two stages: first building the entity graph based on similarity scores, then executing a greedy selection algorithm on the graph structure while using a union-find structure to prevent cycle formation. Finally, it outputs globally optimized entity prediction results for each mention, achieving a balance between local matching and global consistency.

### 2. Main Structure

- **Data Loading Module** 

  `load_scores()` - Loads local similarity scores

  `load_mentions()` - Loads raw mention data

  `load_mentions_expanded()` - Loads LLM-expanded text

- **Similarity Calculation Module**

  `cosine_similarity_text()` - cosine similarity calculation

  `similar_strings()` - Sequence matching similarity

- **Graph Construction Module**

  `build_tree_cover()` - Builds mention-candidate entity heterogeneous graph

  Connection types: mention-entity (similarity edges), entity-entity (semantic edges)

- **Graph Optimization Module**

  `get_maximum_spanning_tree()` - Extracts maximum spanning tree

  `UnionFind` class - Union-find implementation (cycle detection)

- **Core Algorithm Module**

  `greedy_adjustment_with_mst()` - Algorithm 2 greedy selection algorithm

  `compute_entity_coherence()` - Entity consistency score calculation

- **Output Module**

  `save_final_predictions()` - Saves final prediction results

  `main()` - Integrates complete workflow

### 3. Dependencies

```
networkx >= 2.6
scipy >= 1.7
scikit-learn >= 1.0
numpy >= 1.21
Python >= 3.7
```

## PART 5: Evaluation & Visualization

**Script:** `src/eval_metrics.py` 
This module performs the quantitative evaluation of the OpenMEL text-only pipeline, calculating the **Accuracy@1** metric and visualizing the performance gain achieved by the Global Coherence (MST) module.

###  1. Overview
The evaluation script compares the performance of two distinct schemes on the WikiMEL subset:

- **Baseline (LLM + Local):** Ranks candidate entities solely based on the BERT-based local similarity score.
- **Ours (LLM + Local + MST):** Ranks candidate entities based on the final score optimized by the Maximum Spanning Tree (MST) algorithm for global coherence.

### 2. Prerequisites

Ensure the following Python libraries are installed:

```bash
pip install pandas matplotlib
```

### 3. Input & Output Files

The script relies on the standard JSON/JSONL interfaces defined in the project:

| **Type**   | **File Path**                     | **Description**                                              |
| ---------- | --------------------------------- | ------------------------------------------------------------ |
| **Input**  | `data/mentions.json`              | Contains the Ground Truth (`gold_entity_id`) for each mention. |
| **Input**  | `outputs/local_scores.jsonl`      | Output from Part 3 (Local Model scores).                     |
| **Input**  | `outputs/final_predictions.jsonl` | Output from Part 4 (MST Global predictions).                 |
| **Output** | `outputs/accuracy_comparison.png` | Bar chart visualizing the performance comparison.            |
| **Output** | *Console Output*                  | Prints specific accuracy percentages to stdout.              |

### 4. How to Run

Execute the evaluation script from the project root directory:

```bash
python src/eval_metrics.py
```

### 5. Evaluation Logic (Accuracy@1)

The script calculates **Accuracy@1** as follows: 

$$ \text{Accuracy} = \frac{\text{Number of Correctly Linked Mentions}}{\text{Total Number of Mentions}} $$

- **For Baseline:** It selects the candidate with the highest `local_score` from `local_scores.jsonl`. If this candidate matches the `gold_entity_id`, it is counted as correct.
- **For Ours:** It checks the `pred_entity_id` from `final_predictions.jsonl`. If it matches the `gold_entity_id`, it is counted as correct.

### 6. Experimental Results

Based on the WikiMEL test subset, the evaluation yields the following results:

- **Baseline Accuracy:** **50.0%** (6/12)
- **Ours Accuracy (MST):** **83.3%** (10/12)
- **Improvement:** **+33.3%**

The generated visualization (`accuracy_comparison.png`) demonstrates that the **MST Global Optimization** successfully corrects a significant portion of local errors, validating the effectiveness of modeling global entity coherence.
